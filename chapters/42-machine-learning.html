<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Machine Learning and Data Science - Rust Universe: Fearless Systems Engineering</title>


        <!-- Custom HTML head -->

        <meta name="description" content="A comprehensive guide to learning the Rust programming language from fundamentals to mastery.">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../theme/custom.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Rust Universe: Fearless Systems Engineering</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/saeedalam/rust-universe" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        <a href="https://github.com/saeedalam/rust-universe/edit/main/src/chapters/42-machine-learning.md" title="Suggest an edit" aria-label="Suggest an edit">
                            <i id="git-edit-button" class="fa fa-edit"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="chapter-42-machine-learning-and-data-science"><a class="header" href="#chapter-42-machine-learning-and-data-science">Chapter 42: Machine Learning and Data Science</a></h1>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<p>Machine learning (ML) and data science have revolutionized how we extract insights from data and build intelligent systems. Traditionally, languages like Python have dominated these fields due to their rich ecosystem of libraries and tools. However, Rust is making significant inroads into the world of ML and data science, offering performance, safety, and reliability that can be crucial for production systems.</p>
<p>Rust’s strengths—memory safety without garbage collection, concurrency without data races, and performance comparable to C and C++—make it an excellent candidate for computationally intensive and mission-critical ML applications. While Rust’s ML ecosystem is still maturing compared to Python’s, it offers unique advantages for specific use cases, particularly where performance, reliability, and deployment simplicity matter.</p>
<p>In this chapter, we’ll explore how to leverage Rust for machine learning and data science tasks. We’ll cover:</p>
<ul>
<li>Fundamentals of machine learning in Rust</li>
<li>Building efficient data processing pipelines</li>
<li>Interfacing with established ML frameworks</li>
<li>Implementing performance-critical ML algorithms</li>
<li>Developing and deploying ML models</li>
<li>Utilizing GPU acceleration for ML workloads</li>
<li>Integrating with the Python ML ecosystem</li>
</ul>
<p>By the end of this chapter, you’ll have a solid understanding of how to use Rust effectively in ML and data science projects, and you’ll appreciate the unique advantages Rust brings to this domain.</p>
<h2 id="machine-learning-and-data-science-fundamentals"><a class="header" href="#machine-learning-and-data-science-fundamentals">Machine Learning and Data Science Fundamentals</a></h2>
<p>Before diving into Rust-specific implementations, let’s briefly review some key machine learning and data science concepts.</p>
<h3 id="core-ml-concepts"><a class="header" href="#core-ml-concepts">Core ML Concepts</a></h3>
<p>Machine learning is a subset of artificial intelligence focused on building systems that can learn from and make decisions based on data. The main categories of machine learning include:</p>
<ol>
<li><strong>Supervised Learning</strong>: Training models on labeled data to make predictions or classifications</li>
<li><strong>Unsupervised Learning</strong>: Finding patterns or structures in unlabeled data</li>
<li><strong>Reinforcement Learning</strong>: Training agents to make decisions by rewarding desired behaviors</li>
</ol>
<p>Key components of ML systems include:</p>
<ul>
<li><strong>Features</strong>: The input variables used for prediction</li>
<li><strong>Labels</strong>: The output variables the model predicts (in supervised learning)</li>
<li><strong>Training</strong>: The process of optimizing model parameters using data</li>
<li><strong>Inference</strong>: Using a trained model to make predictions on new data</li>
<li><strong>Evaluation</strong>: Assessing model performance using metrics like accuracy, precision, recall, etc.</li>
</ul>
<h3 id="the-ml-workflow-in-rust"><a class="header" href="#the-ml-workflow-in-rust">The ML Workflow in Rust</a></h3>
<p>A typical machine learning workflow in Rust includes:</p>
<ol>
<li><strong>Data Loading and Preprocessing</strong>: Loading data from various sources and preparing it for modeling</li>
<li><strong>Feature Engineering</strong>: Creating and transforming features to improve model performance</li>
<li><strong>Model Training</strong>: Building and optimizing ML models</li>
<li><strong>Model Evaluation</strong>: Assessing model performance</li>
<li><strong>Model Deployment</strong>: Serving model predictions in production</li>
</ol>
<p>Let’s explore how to implement these steps in Rust.</p>
<h2 id="data-processing-in-rust"><a class="header" href="#data-processing-in-rust">Data Processing in Rust</a></h2>
<p>Efficient data processing is the foundation of ML and data science. Let’s look at Rust’s capabilities for working with data.</p>
<h3 id="data-structures-for-ml"><a class="header" href="#data-structures-for-ml">Data Structures for ML</a></h3>
<p>Rust offers several crates for handling the data structures commonly used in ML:</p>
<h4 id="ndarray"><a class="header" href="#ndarray">ndarray</a></h4>
<p>The <code>ndarray</code> crate provides an n-dimensional array type for Rust, similar to NumPy in Python:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use ndarray::{arr1, arr2, Array, Array1, Array2};

fn ndarray_example() {
    // Create a 1D array
    let a = arr1(&amp;[1.0, 2.0, 3.0, 4.0, 5.0]);

    // Create a 2D array
    let b = arr2(&amp;[[1.0, 2.0, 3.0],
                   [4.0, 5.0, 6.0]]);

    // Basic operations
    let c = &amp;a + 1.0;  // Add 1.0 to each element
    let d = &amp;b * 2.0;  // Multiply each element by 2.0

    // Matrix operations
    let e = b.dot(&amp;arr2(&amp;[[1.0], [2.0], [3.0]])); // Matrix multiplication

    println!("a: {}", a);
    println!("b: {}", b);
    println!("c: {}", c);
    println!("d: {}", d);
    println!("e: {}", e);
}
<span class="boring">}</span></code></pre></pre>
<h4 id="polars"><a class="header" href="#polars">polars</a></h4>
<p>The <code>polars</code> crate provides a fast DataFrames library in Rust:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use polars::prelude::*;

fn polars_example() -&gt; Result&lt;(), PolarsError&gt; {
    // Create a DataFrame
    let df = df! [
        "A" =&gt; [1, 2, 3, 4, 5],
        "B" =&gt; [6, 7, 8, 9, 10],
        "C" =&gt; [11, 12, 13, 14, 15]
    ]?;

    println!("{}", df);

    // Basic operations
    let filtered = df.filter(&amp;df["A"].lt(3))?;
    println!("Filtered:\n{}", filtered);

    // Group by and aggregate
    let grouped = df.groupby(["A"])?.agg(&amp;[("B", &amp;["sum", "mean"])])?;
    println!("Grouped:\n{}", grouped);

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="data-loading-and-preprocessing"><a class="header" href="#data-loading-and-preprocessing">Data Loading and Preprocessing</a></h3>
<h4 id="reading-from-various-data-sources"><a class="header" href="#reading-from-various-data-sources">Reading from Various Data Sources</a></h4>
<p>Rust provides crates for reading data from various sources:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::fs::File;
use std::io::BufReader;
use csv::Reader;
use serde::Deserialize;

#[derive(Debug, Deserialize)]
struct Record {
    id: u32,
    feature1: f64,
    feature2: f64,
    label: String,
}

fn load_csv_data() -&gt; Result&lt;Vec&lt;Record&gt;, Box&lt;dyn std::error::Error&gt;&gt; {
    let file = File::open("data.csv")?;
    let reader = BufReader::new(file);
    let mut csv_reader = Reader::from_reader(reader);

    let records: Result&lt;Vec&lt;Record&gt;, _&gt; = csv_reader.deserialize().collect();
    Ok(records?)
}
<span class="boring">}</span></code></pre></pre>
<p>For larger datasets, you might want to use the <code>polars</code> crate for efficient loading:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use polars::prelude::*;

fn load_large_csv() -&gt; Result&lt;DataFrame, PolarsError&gt; {
    CsvReader::from_path("large_data.csv")?
        .has_header(true)
        .finish()
}
<span class="boring">}</span></code></pre></pre>
<h4 id="data-cleaning-and-transformation"><a class="header" href="#data-cleaning-and-transformation">Data Cleaning and Transformation</a></h4>
<p>Data preprocessing is a critical step in ML workflows. Here’s an example using <code>polars</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use polars::prelude::*;

fn preprocess_data(df: &amp;mut DataFrame) -&gt; Result&lt;DataFrame, PolarsError&gt; {
    // Handle missing values
    let df = df.fill_null(FillNullStrategy::Mean)?;

    // Normalize numerical features
    let numeric_cols = vec!["feature1", "feature2", "feature3"];

    let mut processed_df = df.clone();

    for col in numeric_cols {
        let series = df.column(col)?;
        let mean = series.mean().unwrap();
        let std = series.std(1).unwrap();

        let normalized = (series - mean) / std;
        processed_df.replace(col, normalized)?;
    }

    // One-hot encode categorical features
    let dummies = processed_df.columns(["category"])?
        .to_dummies()?;

    // Join the processed data
    processed_df.hstack(&amp;dummies.get_columns())?
}
<span class="boring">}</span></code></pre></pre>
<h3 id="feature-engineering"><a class="header" href="#feature-engineering">Feature Engineering</a></h3>
<p>Feature engineering is the process of creating new features or transforming existing ones to improve model performance. Here’s a simple example:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use ndarray::{Array1, Array2};

fn polynomial_features(x: &amp;Array1&lt;f64&gt;, degree: usize) -&gt; Array2&lt;f64&gt; {
    let n = x.len();
    let mut result = Array2::zeros((n, degree));

    for i in 0..n {
        for j in 0..degree {
            result[[i, j]] = x[i].powi((j + 1) as i32);
        }
    }

    result
}

fn interaction_features(x1: &amp;Array1&lt;f64&gt;, x2: &amp;Array1&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {
    x1 * x2
}
<span class="boring">}</span></code></pre></pre>
<h2 id="building-ml-models-in-rust"><a class="header" href="#building-ml-models-in-rust">Building ML Models in Rust</a></h2>
<p>Now that we understand how to process data in Rust, let’s look at building ML models.</p>
<h3 id="linear-models"><a class="header" href="#linear-models">Linear Models</a></h3>
<p>Linear models are the simplest ML algorithms. Here’s an implementation of linear regression:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use ndarray::{Array1, Array2};
use ndarray_linalg::Solve;

struct LinearRegression {
    coefficients: Array1&lt;f64&gt;,
    intercept: f64,
}

impl LinearRegression {
    fn new() -&gt; Self {
        Self {
            coefficients: Array1::zeros(0),
            intercept: 0.0,
        }
    }

    fn fit(&amp;mut self, x: &amp;Array2&lt;f64&gt;, y: &amp;Array1&lt;f64&gt;) -&gt; Result&lt;(), ndarray_linalg::error::LinalgError&gt; {
        let n_samples = x.nrows();
        let n_features = x.ncols();

        // Add a column of ones for the intercept
        let mut x_with_intercept = Array2::ones((n_samples, n_features + 1));
        x_with_intercept.slice_mut(s![.., 1..]).assign(x);

        // Solve the normal equation: coefficients = (X^T X)^(-1) X^T y
        let xt_x = x_with_intercept.t().dot(&amp;x_with_intercept);
        let xt_y = x_with_intercept.t().dot(y);

        let coefficients = xt_x.solve(&amp;xt_y)?;

        self.intercept = coefficients[0];
        self.coefficients = coefficients.slice(s![1..]).to_owned();

        Ok(())
    }

    fn predict(&amp;self, x: &amp;Array2&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {
        let mut predictions = Array1::from_elem(x.nrows(), self.intercept);
        predictions = predictions + x.dot(&amp;self.coefficients);
        predictions
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="tree-based-models"><a class="header" href="#tree-based-models">Tree-Based Models</a></h3>
<p>Decision trees are popular ML algorithms for both classification and regression. Here’s a simplified implementation:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::collections::HashMap;
use ndarray::{Array1, ArrayView1};

enum SplitRule {
    Continuous { feature_idx: usize, threshold: f64 },
    Categorical { feature_idx: usize, categories: Vec&lt;String&gt; },
}

struct DecisionNode {
    rule: Option&lt;SplitRule&gt;,
    prediction: Option&lt;f64&gt;,
    left: Option&lt;Box&lt;DecisionNode&gt;&gt;,
    right: Option&lt;Box&lt;DecisionNode&gt;&gt;,
}

impl DecisionNode {
    fn new_leaf(prediction: f64) -&gt; Self {
        Self {
            rule: None,
            prediction: Some(prediction),
            left: None,
            right: None,
        }
    }

    fn new_internal(rule: SplitRule, left: DecisionNode, right: DecisionNode) -&gt; Self {
        Self {
            rule: Some(rule),
            prediction: None,
            left: Some(Box::new(left)),
            right: Some(Box::new(right)),
        }
    }

    fn predict(&amp;self, features: &amp;[f64]) -&gt; f64 {
        if let Some(prediction) = self.prediction {
            return prediction;
        }

        match &amp;self.rule {
            Some(SplitRule::Continuous { feature_idx, threshold }) =&gt; {
                let feature_value = features[*feature_idx];
                if feature_value &lt;= *threshold {
                    self.left.as_ref().unwrap().predict(features)
                } else {
                    self.right.as_ref().unwrap().predict(features)
                }
            }
            Some(SplitRule::Categorical { feature_idx, categories }) =&gt; {
                // For simplicity, we assume categorical features are encoded as integers
                let feature_value = features[*feature_idx] as usize;
                if categories.contains(&amp;feature_value.to_string()) {
                    self.left.as_ref().unwrap().predict(features)
                } else {
                    self.right.as_ref().unwrap().predict(features)
                }
            }
            None =&gt; panic!("Decision node without rule or prediction"),
        }
    }
}

struct DecisionTree {
    root: Option&lt;DecisionNode&gt;,
    max_depth: usize,
}

impl DecisionTree {
    fn new(max_depth: usize) -&gt; Self {
        Self {
            root: None,
            max_depth,
        }
    }

    fn fit(&amp;mut self, x: &amp;Array2&lt;f64&gt;, y: &amp;Array1&lt;f64&gt;) {
        let indices: Vec&lt;usize&gt; = (0..x.nrows()).collect();
        self.root = Some(self.build_tree(x, y, &amp;indices, 0));
    }

    fn build_tree(&amp;self, x: &amp;Array2&lt;f64&gt;, y: &amp;Array1&lt;f64&gt;, indices: &amp;[usize], depth: usize) -&gt; DecisionNode {
        // If we reached max depth or only have one sample, create a leaf node
        if depth &gt;= self.max_depth || indices.len() &lt;= 1 {
            let prediction = self.calculate_prediction(y, indices);
            return DecisionNode::new_leaf(prediction);
        }

        // Find the best split
        if let Some((feature_idx, threshold, left_indices, right_indices)) = self.find_best_split(x, y, indices) {
            // If we couldn't split the data further, create a leaf node
            if left_indices.is_empty() || right_indices.is_empty() {
                let prediction = self.calculate_prediction(y, indices);
                return DecisionNode::new_leaf(prediction);
            }

            // Create child nodes recursively
            let left_node = self.build_tree(x, y, &amp;left_indices, depth + 1);
            let right_node = self.build_tree(x, y, &amp;right_indices, depth + 1);

            return DecisionNode::new_internal(
                SplitRule::Continuous { feature_idx, threshold },
                left_node,
                right_node,
            );
        } else {
            // If no good split was found, create a leaf node
            let prediction = self.calculate_prediction(y, indices);
            return DecisionNode::new_leaf(prediction);
        }
    }

    fn find_best_split(&amp;self, x: &amp;Array2&lt;f64&gt;, y: &amp;Array1&lt;f64&gt;, indices: &amp;[usize]) -&gt; Option&lt;(usize, f64, Vec&lt;usize&gt;, Vec&lt;usize&gt;)&gt; {
        let n_features = x.ncols();
        let n_samples = indices.len();

        let mut best_gain = f64::NEG_INFINITY;
        let mut best_feature = 0;
        let mut best_threshold = 0.0;
        let mut best_left_indices = Vec::new();
        let mut best_right_indices = Vec::new();

        // Calculate current impurity
        let current_impurity = self.calculate_impurity(y, indices);

        // Try each feature
        for feature_idx in 0..n_features {
            // Get unique values for this feature
            let mut feature_values = Vec::with_capacity(n_samples);
            for &amp;idx in indices {
                feature_values.push(x[[idx, feature_idx]]);
            }
            feature_values.sort_by(|a, b| a.partial_cmp(b).unwrap());

            // Try each threshold
            for i in 0..feature_values.len() - 1 {
                let threshold = (feature_values[i] + feature_values[i + 1]) / 2.0;

                let mut left_indices = Vec::new();
                let mut right_indices = Vec::new();

                // Split data based on threshold
                for &amp;idx in indices {
                    if x[[idx, feature_idx]] &lt;= threshold {
                        left_indices.push(idx);
                    } else {
                        right_indices.push(idx);
                    }
                }

                // Skip if split is degenerate
                if left_indices.is_empty() || right_indices.is_empty() {
                    continue;
                }

                // Calculate impurity for children
                let left_impurity = self.calculate_impurity(y, &amp;left_indices);
                let right_impurity = self.calculate_impurity(y, &amp;right_indices);

                // Calculate information gain
                let left_weight = left_indices.len() as f64 / n_samples as f64;
                let right_weight = right_indices.len() as f64 / n_samples as f64;
                let gain = current_impurity - (left_weight * left_impurity + right_weight * right_impurity);

                // Update best split if this one is better
                if gain &gt; best_gain {
                    best_gain = gain;
                    best_feature = feature_idx;
                    best_threshold = threshold;
                    best_left_indices = left_indices;
                    best_right_indices = right_indices;
                }
            }
        }

        if best_gain &gt; 0.0 {
            Some((best_feature, best_threshold, best_left_indices, best_right_indices))
        } else {
            None
        }
    }

    fn calculate_impurity(&amp;self, y: &amp;Array1&lt;f64&gt;, indices: &amp;[usize]) -&gt; f64 {
        // For regression, we use variance as impurity
        if indices.is_empty() {
            return 0.0;
        }

        let mean = indices.iter().map(|&amp;i| y[i]).sum::&lt;f64&gt;() / indices.len() as f64;
        let variance = indices.iter().map(|&amp;i| (y[i] - mean).powi(2)).sum::&lt;f64&gt;() / indices.len() as f64;

        variance
    }

    fn calculate_prediction(&amp;self, y: &amp;Array1&lt;f64&gt;, indices: &amp;[usize]) -&gt; f64 {
        // For regression, prediction is the mean of target values
        if indices.is_empty() {
            return 0.0;
        }

        indices.iter().map(|&amp;i| y[i]).sum::&lt;f64&gt;() / indices.len() as f64
    }

    fn predict(&amp;self, x: &amp;Array2&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {
        let n_samples = x.nrows();
        let mut predictions = Array1::zeros(n_samples);

        for i in 0..n_samples {
            let features = x.row(i).to_vec();
            predictions[i] = self.root.as_ref().unwrap().predict(&amp;features);
        }

        predictions
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="using-existing-ml-crates"><a class="header" href="#using-existing-ml-crates">Using Existing ML Crates</a></h3>
<p>While implementing ML algorithms from scratch is educational, in practice, you’ll often use existing libraries. Let’s look at some Rust ML crates:</p>
<h4 id="linfa"><a class="header" href="#linfa">linfa</a></h4>
<p>The <code>linfa</code> crate is a collection of ML algorithms written in Rust:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use linfa::prelude::*;
use linfa_linear::LinearRegression;
use ndarray::Array2;

fn linfa_example() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    // Load or create your dataset
    let (train_features, train_labels) = load_dataset()?;

    // Create a dataset
    let dataset = Dataset::new(train_features, train_labels);

    // Train a linear regression model
    let model = LinearRegression::default()
        .fit(&amp;dataset)?;

    // Make predictions
    let test_features = Array2::ones((10, 3));
    let predictions = model.predict(&amp;test_features);

    println!("Predictions: {:?}", predictions);
    println!("Model coefficients: {:?}", model.params());

    Ok(())
}

fn load_dataset() -&gt; Result&lt;(Array2&lt;f64&gt;, Array1&lt;f64&gt;), Box&lt;dyn std::error::Error&gt;&gt; {
    // In a real application, load and preprocess your data here
    let features = Array2::ones((100, 3));
    let labels = Array1::ones(100);

    Ok((features, labels))
}
<span class="boring">}</span></code></pre></pre>
<h4 id="smartcore"><a class="header" href="#smartcore">smartcore</a></h4>
<p>The <code>smartcore</code> crate is another ML library for Rust:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use smartcore::linalg::basic::matrix::DenseMatrix;
use smartcore::linear::linear_regression::LinearRegression;

fn smartcore_example() {
    // Create a dataset
    let x = DenseMatrix::from_2d_array(&amp;[
        &amp;[1.0, 2.0],
        &amp;[3.0, 4.0],
        &amp;[5.0, 6.0],
        &amp;[7.0, 8.0],
        &amp;[9.0, 10.0],
    ]);
    let y = vec![2.0, 4.0, 6.0, 8.0, 10.0];

    // Fit a linear regression model
    let model = LinearRegression::fit(&amp;x, &amp;y, Default::default()).unwrap();

    // Make predictions
    let x_test = DenseMatrix::from_2d_array(&amp;[
        &amp;[11.0, 12.0],
        &amp;[13.0, 14.0],
    ]);
    let predictions = model.predict(&amp;x_test).unwrap();

    println!("Predictions: {:?}", predictions);
}
<span class="boring">}</span></code></pre></pre>
<p>By using these libraries, you can implement ML models more efficiently while still leveraging Rust’s performance and safety benefits.</p>
<h2 id="interfacing-with-ml-frameworks"><a class="header" href="#interfacing-with-ml-frameworks">Interfacing with ML Frameworks</a></h2>
<p>While Rust’s native ML ecosystem is growing, you might need to interface with established ML frameworks written in other languages. Let’s explore how to do this effectively.</p>
<h3 id="rust-and-python-integration"><a class="header" href="#rust-and-python-integration">Rust and Python Integration</a></h3>
<p>Python has a rich ecosystem of ML libraries like TensorFlow, PyTorch, and scikit-learn. You can interface with these libraries from Rust using crates like <code>pyo3</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pyo3::prelude::*;
use pyo3::types::{PyList, PyDict};
use ndarray::Array2;

fn use_sklearn_from_rust() -&gt; PyResult&lt;()&gt; {
    Python::with_gil(|py| {
        // Import Python modules
        let sklearn = py.import("sklearn.ensemble")?;
        let np = py.import("numpy")?;

        // Create NumPy arrays for data
        let x_data = np.call_method1("array", (vec![
            vec![1.0, 2.0, 3.0],
            vec![4.0, 5.0, 6.0],
            vec![7.0, 8.0, 9.0],
        ],))?;

        let y_data = np.call_method1("array", (vec![0, 1, 1],))?;

        // Create and train a random forest classifier
        let rf = sklearn.call_method1("RandomForestClassifier", (10,))?;
        rf.call_method1("fit", (x_data, y_data))?;

        // Make predictions
        let x_test = np.call_method1("array", (vec![
            vec![3.0, 5.0, 7.0],
        ],))?;

        let predictions = rf.call_method1("predict", (x_test,))?;
        println!("Predictions: {:?}", predictions);

        // Get feature importances
        let importances = rf.getattr("feature_importances_")?;
        println!("Feature importances: {:?}", importances);

        Ok(())
    })
}
<span class="boring">}</span></code></pre></pre>
<h3 id="tensorflow-and-rust"><a class="header" href="#tensorflow-and-rust">TensorFlow and Rust</a></h3>
<p>You can use TensorFlow models in Rust using the <code>tensorflow</code> crate:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tensorflow::{Graph, ImportGraphDefOptions, Session, SessionOptions, SessionRunArgs, Tensor};
use std::fs::File;
use std::io::Read;

fn use_tensorflow_model() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    // Load a pre-trained model
    let mut model_data = Vec::new();
    File::open("model.pb")?.read_to_end(&amp;mut model_data)?;

    // Create TensorFlow graph and session
    let mut graph = Graph::new();
    graph.import_graph_def(&amp;model_data, &amp;ImportGraphDefOptions::new())?;
    let session = Session::new(&amp;SessionOptions::new(), &amp;graph)?;

    // Prepare input data
    let input_data: Vec&lt;f32&gt; = vec![1.0, 2.0, 3.0, 4.0];
    let input_tensor = Tensor::new(&amp;[1, 4]).with_values(&amp;input_data)?;

    // Run inference
    let mut args = SessionRunArgs::new();
    let input_op = graph.operation_by_name_required("input_op")?;
    let output_op = graph.operation_by_name_required("output_op")?;

    args.add_feed(&amp;input_op, 0, &amp;input_tensor);
    let output_fetch = args.request_fetch(&amp;output_op, 0);

    session.run(&amp;mut args)?;

    // Get results
    let output_tensor = args.fetch::&lt;f32&gt;(output_fetch)?;
    let output_data = output_tensor.to_vec();
    println!("Model output: {:?}", output_data);

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="onnx-and-rust"><a class="header" href="#onnx-and-rust">ONNX and Rust</a></h3>
<p>ONNX (Open Neural Network Exchange) is a format for representing ML models that allows for interoperability between different frameworks. The <code>tract</code> crate provides ONNX support in Rust:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tract_onnx::prelude::*;

fn use_onnx_model() -&gt; TractResult&lt;()&gt; {
    // Load the ONNX model
    let model = tract_onnx::onnx()
        .model_for_path("model.onnx")?
        .with_input_fact(0, InferenceFact::dt_shape(f32::datum_type(), tvec!(1, 3, 224, 224)))?
        .into_optimized()?
        .into_runnable()?;

    // Prepare input data (example: random tensor for a 224x224 RGB image)
    let input_data = tract_ndarray::Array4::from_shape_fn((1, 3, 224, 224), |_| -&gt; f32 { rand::random() });

    // Run inference
    let result = model.run(tvec!(input_data.into()))?;

    // Process the output
    let output_tensor = result[0].to_array_view::&lt;f32&gt;()?;
    let best_class_idx = output_tensor
        .iter()
        .enumerate()
        .max_by(|a, b| a.1.partial_cmp(b.1).unwrap())
        .map(|(idx, _)| idx)
        .unwrap();

    println!("Predicted class: {}", best_class_idx);

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h2 id="performance-critical-ml-algorithms"><a class="header" href="#performance-critical-ml-algorithms">Performance-Critical ML Algorithms</a></h2>
<p>One of Rust’s strengths is its performance, making it ideal for implementing performance-critical ML algorithms. Let’s explore some examples.</p>
<h3 id="k-means-clustering"><a class="header" href="#k-means-clustering">K-Means Clustering</a></h3>
<p>K-means is a popular unsupervised learning algorithm for clustering:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use ndarray::{Array1, Array2, Axis};
use rand::seq::SliceRandom;
use rand::thread_rng;
use std::f64;

struct KMeans {
    k: usize,
    max_iterations: usize,
    centroids: Option&lt;Array2&lt;f64&gt;&gt;,
}

impl KMeans {
    fn new(k: usize, max_iterations: usize) -&gt; Self {
        Self {
            k,
            max_iterations,
            centroids: None,
        }
    }

    fn fit(&amp;mut self, x: &amp;Array2&lt;f64&gt;) -&gt; Array1&lt;usize&gt; {
        let n_samples = x.nrows();
        let n_features = x.ncols();

        // Initialize centroids using k-means++
        let mut centroids = Array2::zeros((self.k, n_features));
        let mut rng = thread_rng();

        // Choose first centroid randomly
        let first_centroid_idx = (0..n_samples).choose(&amp;mut rng).unwrap();
        centroids.row_mut(0).assign(&amp;x.row(first_centroid_idx));

        // Choose remaining centroids with probability proportional to distance
        for i in 1..self.k {
            let mut distances = Array1::zeros(n_samples);

            for j in 0..n_samples {
                let mut min_dist = f64::INFINITY;

                for c in 0..i {
                    let dist = euclidean_distance(&amp;x.row(j), &amp;centroids.row(c));
                    min_dist = min_dist.min(dist);
                }

                distances[j] = min_dist;
            }

            // Normalize distances to create a probability distribution
            let sum_distances = distances.sum();
            let probs = &amp;distances / sum_distances;

            // Choose next centroid based on probability
            let mut cumsum = 0.0;
            let rand_val = rand::random::&lt;f64&gt;();
            let mut next_centroid_idx = 0;

            for j in 0..n_samples {
                cumsum += probs[j];
                if cumsum &gt;= rand_val {
                    next_centroid_idx = j;
                    break;
                }
            }

            centroids.row_mut(i).assign(&amp;x.row(next_centroid_idx));
        }

        // Iterative k-means algorithm
        let mut labels = Array1::zeros(n_samples);
        let mut prev_centroids = Array2::zeros((self.k, n_features));

        for _ in 0..self.max_iterations {
            // Assign points to nearest centroid
            for i in 0..n_samples {
                let mut min_dist = f64::INFINITY;
                let mut closest_centroid = 0;

                for j in 0..self.k {
                    let dist = euclidean_distance(&amp;x.row(i), &amp;centroids.row(j));
                    if dist &lt; min_dist {
                        min_dist = dist;
                        closest_centroid = j;
                    }
                }

                labels[i] = closest_centroid;
            }

            // Save previous centroids to check for convergence
            prev_centroids.assign(&amp;centroids);

            // Update centroids based on assigned points
            for j in 0..self.k {
                let mut sum = Array1::zeros(n_features);
                let mut count = 0;

                for i in 0..n_samples {
                    if labels[i] == j {
                        sum = &amp;sum + &amp;x.row(i);
                        count += 1;
                    }
                }

                if count &gt; 0 {
                    centroids.row_mut(j).assign(&amp;(&amp;sum / count as f64));
                }
            }

            // Check for convergence
            if has_converged(&amp;prev_centroids, &amp;centroids) {
                break;
            }
        }

        self.centroids = Some(centroids);
        labels
    }

    fn predict(&amp;self, x: &amp;Array2&lt;f64&gt;) -&gt; Array1&lt;usize&gt; {
        let centroids = self.centroids.as_ref().expect("Model not fitted yet");
        let n_samples = x.nrows();
        let mut labels = Array1::zeros(n_samples);

        for i in 0..n_samples {
            let mut min_dist = f64::INFINITY;
            let mut closest_centroid = 0;

            for j in 0..self.k {
                let dist = euclidean_distance(&amp;x.row(i), &amp;centroids.row(j));
                if dist &lt; min_dist {
                    min_dist = dist;
                    closest_centroid = j;
                }
            }

            labels[i] = closest_centroid;
        }

        labels
    }
}

fn euclidean_distance(a: &amp;ndarray::ArrayView1&lt;f64&gt;, b: &amp;ndarray::ArrayView1&lt;f64&gt;) -&gt; f64 {
    a.iter()
        .zip(b.iter())
        .map(|(&amp;x, &amp;y)| (x - y).powi(2))
        .sum::&lt;f64&gt;()
        .sqrt()
}

fn has_converged(prev_centroids: &amp;Array2&lt;f64&gt;, centroids: &amp;Array2&lt;f64&gt;) -&gt; bool {
    let tolerance = 1e-4;
    let max_diff = prev_centroids
        .iter()
        .zip(centroids.iter())
        .map(|(&amp;x, &amp;y)| (x - y).abs())
        .fold(0.0, |acc, x| acc.max(x));

    max_diff &lt; tolerance
}
<span class="boring">}</span></code></pre></pre>
<h3 id="gradient-boosting"><a class="header" href="#gradient-boosting">Gradient Boosting</a></h3>
<p>Gradient boosting is a powerful ML technique that builds an ensemble of weak prediction models:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use ndarray::{Array1, Array2};

struct GradientBoostingRegressor {
    n_estimators: usize,
    learning_rate: f64,
    max_depth: usize,
    trees: Vec&lt;DecisionTree&gt;,
    initial_prediction: f64,
}

impl GradientBoostingRegressor {
    fn new(n_estimators: usize, learning_rate: f64, max_depth: usize) -&gt; Self {
        Self {
            n_estimators,
            learning_rate,
            max_depth,
            trees: Vec::with_capacity(n_estimators),
            initial_prediction: 0.0,
        }
    }

    fn fit(&amp;mut self, x: &amp;Array2&lt;f64&gt;, y: &amp;Array1&lt;f64&gt;) {
        // Initialize with mean prediction
        self.initial_prediction = y.mean().unwrap_or(0.0);
        let mut current_predictions = Array1::from_elem(x.nrows(), self.initial_prediction);

        // Iteratively build trees
        for _ in 0..self.n_estimators {
            // Calculate pseudo-residuals
            let residuals = y - &amp;current_predictions;

            // Train a tree on the residuals
            let mut tree = DecisionTree::new(self.max_depth);
            tree.fit(x, &amp;residuals);

            // Update predictions
            let tree_predictions = tree.predict(x);
            current_predictions = &amp;current_predictions + &amp;(&amp;tree_predictions * self.learning_rate);

            // Store the tree
            self.trees.push(tree);
        }
    }

    fn predict(&amp;self, x: &amp;Array2&lt;f64&gt;) -&gt; Array1&lt;f64&gt; {
        // Start with initial prediction
        let mut predictions = Array1::from_elem(x.nrows(), self.initial_prediction);

        // Add contributions from each tree
        for tree in &amp;self.trees {
            predictions = &amp;predictions + &amp;(&amp;tree.predict(x) * self.learning_rate);
        }

        predictions
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="gpu-acceleration-for-ml-workloads"><a class="header" href="#gpu-acceleration-for-ml-workloads">GPU Acceleration for ML Workloads</a></h2>
<p>Leveraging GPU acceleration is essential for many ML workloads, especially deep learning. Rust provides several crates for GPU programming:</p>
<h3 id="cuda-integration"><a class="header" href="#cuda-integration">CUDA Integration</a></h3>
<p>The <code>rust-cuda</code> ecosystem allows you to write CUDA kernels directly in Rust:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use rustacuda::prelude::*;
use rustacuda::memory::DeviceBox;

fn cuda_example() -&gt; Result&lt;(), rustacuda::error::CudaError&gt; {
    // Initialize CUDA
    rustacuda::init(CudaFlags::empty())?;

    // Get the first device
    let device = Device::get_device(0)?;

    // Create a context
    let _context = Context::create_and_push(
        ContextFlags::MAP_HOST | ContextFlags::SCHED_AUTO, device)?;

    // Create data
    let mut host_data = [1.0f32, 2.0, 3.0, 4.0, 5.0];
    let mut device_data = DeviceBox::new(&amp;host_data)?;

    // Load and compile the kernel
    let module = Module::load_from_string(&amp;include_str!("kernel.ptx"))?;

    // Launch the kernel
    let stream = Stream::new(StreamFlags::NON_BLOCKING, None)?;
    unsafe {
        launch!(module.multiply_by_2 &lt;&lt;&lt;1, host_data.len() as u32, 0, stream&gt;&gt;&gt;(
            device_data.as_device_ptr(),
            host_data.len()
        ))?;
    }

    // Copy the result back
    device_data.copy_to(&amp;mut host_data)?;

    println!("Result: {:?}", host_data);

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="gpu-computing-with-opencl"><a class="header" href="#gpu-computing-with-opencl">GPU Computing with OpenCL</a></h3>
<p>For more portable GPU computing, you can use OpenCL via the <code>ocl</code> crate:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use ocl::{ProQue, Buffer, MemFlags};

fn opencl_example() -&gt; ocl::Result&lt;()&gt; {
    // OpenCL kernel as a string
    let src = r#"
        __kernel void multiply_by_2(__global float* data) {
            size_t idx = get_global_id(0);
            data[idx] *= 2.0f;
        }
    "#;

    // Initialize OpenCL
    let pro_que = ProQue::builder()
        .src(src)
        .dims(5) // 5 work items
        .build()?;

    // Create a buffer
    let mut data = vec![1.0f32, 2.0, 3.0, 4.0, 5.0];
    let buffer = Buffer::builder()
        .queue(pro_que.queue().clone())
        .flags(MemFlags::READ_WRITE)
        .len(5)
        .copy_host_slice(&amp;data)
        .build()?;

    // Create and enqueue the kernel
    let kernel = pro_que.kernel_builder("multiply_by_2")
        .arg(&amp;buffer)
        .build()?;

    unsafe { kernel.enq()? }

    // Read the results
    buffer.read(&amp;mut data).enq()?;

    println!("Result: {:?}", data);

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="gpu-accelerated-neural-networks"><a class="header" href="#gpu-accelerated-neural-networks">GPU-Accelerated Neural Networks</a></h3>
<p>For neural networks, you can use crates like <code>tch-rs</code> (PyTorch bindings for Rust):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tch::{nn, Device, Tensor};

fn neural_network_example() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    // Check if CUDA is available
    let device = if tch::Cuda::is_available() {
        Device::Cuda(0)
    } else {
        Device::Cpu
    };

    // Create a simple neural network
    let vs = nn::VarStore::new(device);
    let net = nn::seq()
        .add(nn::linear(&amp;vs.root(), 784, 128, Default::default()))
        .add_fn(|x| x.relu())
        .add(nn::linear(&amp;vs.root(), 128, 10, Default::default()));

    // Create some random input
    let x = Tensor::rand(&amp;[64, 784], (tch::Kind::Float, device));

    // Forward pass
    let y = net.forward(&amp;x);

    println!("Input shape: {:?}", x.size());
    println!("Output shape: {:?}", y.size());

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h2 id="modern-rust-ml-frameworks"><a class="header" href="#modern-rust-ml-frameworks">Modern Rust ML Frameworks</a></h2>
<p>Rust’s ML ecosystem has grown significantly in recent years, with several promising frameworks emerging. Let’s explore some of the most notable ones.</p>
<h3 id="burn-a-modern-deep-learning-framework"><a class="header" href="#burn-a-modern-deep-learning-framework">Burn: A Modern Deep Learning Framework</a></h3>
<p>Burn is a modern deep learning framework written in Rust that offers strong GPU acceleration, automatic differentiation, and high-performance neural network implementations.</p>
<p>Key features of Burn include:</p>
<ol>
<li><strong>Type-safety</strong>: Burn leverages Rust’s type system to catch errors at compile time</li>
<li><strong>Backend Agnostic</strong>: Supports CPU, CUDA, and other backends</li>
<li><strong>Dynamic Computation Graph</strong>: Allows for flexible model architectures</li>
<li><strong>High Performance</strong>: Optimized for both training and inference</li>
</ol>
<p>Here’s a simple example of using Burn to create and train a neural network:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use burn::{
    config::Config,
    module::{Module, ModuleT},
    nn::{
        conv::{Conv2d, Conv2dConfig},
        linear::{Linear, LinearConfig},
        pool::{AdaptiveAvgPool2d, AdaptiveAvgPool2dConfig},
    },
    tensor::{backend::Backend, Tensor},
};

// Define the model architecture
#[derive(Module, Debug)]
struct SimpleCNN&lt;B: Backend&gt; {
    conv1: Conv2d&lt;B&gt;,
    conv2: Conv2d&lt;B&gt;,
    pool: AdaptiveAvgPool2d,
    fc1: Linear&lt;B&gt;,
    fc2: Linear&lt;B&gt;,
}

// Configuration for the model
#[derive(Config, Debug)]
struct SimpleCNNConfig {
    conv1: Conv2dConfig,
    conv2: Conv2dConfig,
    pool: AdaptiveAvgPool2dConfig,
    fc1: LinearConfig,
    fc2: LinearConfig,
}

impl&lt;B: Backend&gt; ModuleT&lt;B&gt; for SimpleCNN&lt;B&gt; {
    type Config = SimpleCNNConfig;

    fn new(config: &amp;Self::Config, device: &amp;B::Device) -&gt; Self {
        Self {
            conv1: Conv2d::new(config.conv1.clone(), device),
            conv2: Conv2d::new(config.conv2.clone(), device),
            pool: AdaptiveAvgPool2d::new(config.pool.clone()),
            fc1: Linear::new(config.fc1.clone(), device),
            fc2: Linear::new(config.fc2.clone(), device),
        }
    }

    fn forward(&amp;self, x: Tensor&lt;B, 4&gt;) -&gt; Tensor&lt;B, 2&gt; {
        // Forward pass through convolutional layers
        let x = self.conv1.forward(x).relu();
        let x = self.conv2.forward(x).relu();

        // Apply pooling
        let x = self.pool.forward(x);

        // Flatten and pass through fully connected layers
        let batch_size = x.dims()[0];
        let x = x.reshape([batch_size, -1]);
        let x = self.fc1.forward(x).relu();
        self.fc2.forward(x)
    }
}

// Create a model configuration
fn create_model_config() -&gt; SimpleCNNConfig {
    SimpleCNNConfig {
        conv1: Conv2dConfig::new([3, 16], [3, 3]),
        conv2: Conv2dConfig::new([16, 32], [3, 3]),
        pool: AdaptiveAvgPool2dConfig::new([1, 1]),
        fc1: LinearConfig::new(32, 64),
        fc2: LinearConfig::new(64, 10),
    }
}

// Example of training the model (simplified)
fn train_example&lt;B: Backend&gt;() {
    // Create the model
    let config = create_model_config();
    let device = B::Device::default();
    let model = SimpleCNN::new(&amp;config, &amp;device);

    // Define optimizer, loss function, dataset, etc.
    // ...

    // Training loop would go here
    // ...
}
<span class="boring">}</span></code></pre></pre>
<h3 id="candle-for-foundation-models"><a class="header" href="#candle-for-foundation-models">Candle: For Foundation Models</a></h3>
<p>Candle is a minimalist ML framework focused on running foundation models (like LLMs) efficiently. It’s designed for inference rather than training and is optimized for production deployments.</p>
<p>Key features of Candle include:</p>
<ol>
<li><strong>Minimal Dependencies</strong>: Self-contained with few external dependencies</li>
<li><strong>CUDA and Metal Support</strong>: Efficient GPU acceleration on multiple platforms</li>
<li><strong>Quantization Support</strong>: 4-bit and 8-bit quantization for efficient inference</li>
<li><strong>Model Compatibility</strong>: Easy loading of models from Hugging Face and other sources</li>
</ol>
<p>Here’s how to load and run an LLM with Candle:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use candle::{DType, Device, Tensor};
use candle_nn::{ops, VarBuilder};
use candle_transformers::models::llama::{Config, Llama};

// Load a pre-trained LLaMA model
fn load_llama_model() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    // Select device (CUDA if available, otherwise CPU)
    let device = if candle::cuda_is_available() {
        Device::Cuda(0)
    } else {
        Device::Cpu
    };

    // Load model configuration
    let config = Config::default();

    // Load weights from disk
    let vb = VarBuilder::from_saved("path/to/model/weights", DType::F16, &amp;device)?;

    // Initialize the model
    let model = Llama::new(&amp;config, vb)?;

    // Tokenize input
    let tokens = vec![1, 2, 3, 4]; // Example token IDs
    let input = Tensor::new(tokens, &amp;device)?;

    // Run inference
    let logits = model.forward(&amp;input)?;

    // Process outputs
    let next_token = ops::argmax(&amp;logits.i([(tokens.len() - 1)..])?, -1)?;
    println!("Next token: {:?}", next_token);

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="linfa-for-traditional-ml-algorithms"><a class="header" href="#linfa-for-traditional-ml-algorithms">Linfa: For Traditional ML Algorithms</a></h3>
<p>Linfa is Rust’s answer to scikit-learn, providing implementations of traditional machine learning algorithms:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use linfa::prelude::*;
use linfa_clustering::KMeans;
use ndarray::{array, Array2};

fn kmeans_example() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    // Create some sample data
    let data = array![
        [1.0, 2.0],
        [1.1, 2.1],
        [1.2, 2.2],
        [5.0, 6.0],
        [5.1, 6.1],
        [5.2, 6.2],
    ];

    // Convert to a dataset
    let dataset = Dataset::from(data.clone())
        .with_feature_names(vec!["x".to_string(), "y".to_string()]);

    // Run K-means clustering with k=2
    let model = KMeans::params(2)
        .max_n_iterations(100)
        .tolerance(1e-5)
        .fit(&amp;dataset)?;

    // Get cluster assignments
    let labels = model.predict(&amp;dataset);

    // Get cluster centers
    let centroids = model.centroids();

    println!("Labels: {:?}", labels);
    println!("Centroids: {:?}", centroids);

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h2 id="integration-with-python-ml-ecosystem"><a class="header" href="#integration-with-python-ml-ecosystem">Integration with Python ML Ecosystem</a></h2>
<p>While Rust’s ML ecosystem is growing, Python remains the dominant language for ML due to its extensive libraries like TensorFlow, PyTorch, scikit-learn, and more. Fortunately, Rust provides excellent tools for integrating with Python’s ML ecosystem.</p>
<h3 id="pyo3-for-seamless-interoperability"><a class="header" href="#pyo3-for-seamless-interoperability">PyO3 for Seamless Interoperability</a></h3>
<p>PyO3 allows you to create Python bindings for Rust code and call Python functions from Rust:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use pyo3::prelude::*;
use pyo3::types::{PyList, PyDict};

// Function to call Python's scikit-learn from Rust
fn scikit_learn_from_rust() -&gt; PyResult&lt;()&gt; {
    Python::with_gil(|py| {
        // Import Python modules
        let sklearn = py.import("sklearn.ensemble")?;
        let np = py.import("numpy")?;

        // Create sample data
        let x = np.call_method1("array", ([
            [1.0, 2.0],
            [2.0, 3.0],
            [3.0, 4.0],
            [4.0, 5.0],
        ],))?;

        let y = np.call_method1("array", ([0, 0, 1, 1],))?;

        // Create a random forest classifier
        let rf = sklearn.call_method1("RandomForestClassifier", ())?;

        // Train the model
        rf.call_method1("fit", (x, y))?;

        // Make predictions
        let x_test = np.call_method1("array", ([[2.5, 3.5], [3.5, 4.5]],))?;
        let predictions = rf.call_method1("predict", (x_test,))?;

        println!("Predictions: {:?}", predictions);

        Ok(())
    })
}

// Function to expose Rust code to Python
#[pyfunction]
fn process_data(data: &amp;PyList) -&gt; PyResult&lt;PyObject&gt; {
    let gil = Python::acquire_gil();
    let py = gil.python();

    // Convert Python list to Rust Vec
    let mut rust_data: Vec&lt;f64&gt; = data.extract()?;

    // Process data in Rust (e.g., normalize)
    let sum: f64 = rust_data.iter().sum();
    let mean = sum / rust_data.len() as f64;

    for val in &amp;mut rust_data {
        *val = *val / mean;
    }

    // Convert back to Python
    let result = PyList::new(py, &amp;rust_data);
    Ok(result.into())
}

// Module definition for Python bindings
#[pymodule]
fn rust_ml_helpers(_py: Python, m: &amp;PyModule) -&gt; PyResult&lt;()&gt; {
    m.add_function(wrap_pyfunction!(process_data, m)?)?;
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="calling-tensorflow-and-pytorch-from-rust"><a class="header" href="#calling-tensorflow-and-pytorch-from-rust">Calling TensorFlow and PyTorch from Rust</a></h3>
<p>For deep learning with TensorFlow or PyTorch, you can use their respective Rust bindings:</p>
<h4 id="tensorflow-with-tensorflow-rust"><a class="header" href="#tensorflow-with-tensorflow-rust">TensorFlow with tensorflow-rust:</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tensorflow::{Graph, ImportGraphDefOptions, Session, SessionOptions, Status, Tensor};

fn tensorflow_inference() -&gt; Result&lt;(), Status&gt; {
    // Load a pre-trained model
    let mut graph = Graph::new();
    let model_data = std::fs::read("model.pb")?;
    graph.import_graph_def(&amp;model_data, &amp;ImportGraphDefOptions::new())?;

    // Create a session
    let session = Session::new(&amp;SessionOptions::new(), &amp;graph)?;

    // Prepare input
    let input_data: Vec&lt;f32&gt; = vec![1.0, 2.0, 3.0, 4.0];
    let input_tensor = Tensor::new(&amp;[1, 4]).with_values(&amp;input_data)?;

    // Run inference
    let mut input_tensors = vec![input_tensor];
    let output_tensors = session.run(
        &amp;[],
        &amp;[("input", &amp;input_tensors[0])],
        &amp;["output"],
        None,
    )?;

    // Process results
    let output: &amp;Tensor&lt;f32&gt; = &amp;output_tensors[0];
    println!("Output shape: {:?}", output.dims());

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h4 id="pytorch-with-tch-rs"><a class="header" href="#pytorch-with-tch-rs">PyTorch with tch-rs:</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tch::{nn, Device, Tensor};
use std::path::Path;

fn pytorch_model_inference() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    // Load a TorchScript model
    let model_path = Path::new("model.pt");
    let model = CModule::load(model_path)?;

    // Prepare input tensor
    let input = Tensor::of_slice(&amp;[1.0f32, 2.0, 3.0, 4.0])
        .view((1, 4)); // Reshape to batch_size=1, features=4

    // Run inference
    let output = model.forward_ts(&amp;[input])?;

    println!("Output: {:?}", output);

    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="building-hybrid-rust-python-ml-pipelines"><a class="header" href="#building-hybrid-rust-python-ml-pipelines">Building Hybrid Rust-Python ML Pipelines</a></h3>
<p>For production ML systems, a common pattern is to use Python for training and Rust for deployment:</p>
<ol>
<li><strong>Train in Python</strong>: Leverage Python’s rich ecosystem for data exploration, model development, and training</li>
<li><strong>Export the Model</strong>: Save the trained model in a format that can be loaded in Rust</li>
<li><strong>Deploy with Rust</strong>: Build a high-performance, memory-safe inference service in Rust</li>
</ol>
<p>This approach combines the best of both worlds:</p>
<pre><pre class="playground"><code class="language-rust">use actix_web::{web, App, HttpResponse, HttpServer, Responder};
use serde::{Deserialize, Serialize};
use tch::{CModule, Tensor};

// Load the PyTorch model (trained in Python)
static MODEL: once_cell::sync::Lazy&lt;CModule&gt; = once_cell::sync::Lazy::new(|| {
    CModule::load("model.pt").expect("Failed to load model")
});

// Request and response types
#[derive(Deserialize)]
struct PredictionRequest {
    features: Vec&lt;f32&gt;,
}

#[derive(Serialize)]
struct PredictionResponse {
    prediction: f32,
    confidence: f32,
}

// API endpoint for predictions
async fn predict(request: web::Json&lt;PredictionRequest&gt;) -&gt; impl Responder {
    // Convert input to tensor
    let input = Tensor::of_slice(&amp;request.features)
        .view((1, request.features.len() as i64));

    // Run inference
    let output = MODEL.forward_ts(&amp;[input])
        .expect("Model inference failed");

    // Extract prediction and confidence
    let values = output.to_kind(tch::Kind::Float).try_into::&lt;Vec&lt;f32&gt;&gt;()
        .expect("Failed to convert output to Vec");

    let prediction = values[0];
    let confidence = values[1];

    // Return JSON response
    HttpResponse::Ok().json(PredictionResponse {
        prediction,
        confidence,
    })
}

// Main function to run the server
#[actix_web::main]
async fn main() -&gt; std::io::Result&lt;()&gt; {
    HttpServer::new(|| {
        App::new()
            .route("/predict", web::post().to(predict))
    })
    .bind("127.0.0.1:8080")?
    .run()
    .await
}</code></pre></pre>
<h3 id="deploying-python-models-with-rust-services"><a class="header" href="#deploying-python-models-with-rust-services">Deploying Python Models with Rust Services</a></h3>
<p>For more complex scenarios where you need to keep Python in the deployment stack, you can use PyO3 to embed a Python interpreter within your Rust application:</p>
<pre><pre class="playground"><code class="language-rust">use pyo3::prelude::*;
use pyo3::types::IntoPyDict;
use actix_web::{web, App, HttpResponse, HttpServer, Responder};
use serde::{Deserialize, Serialize};

// Shared Python interpreter state
struct PythonModel {
    model: PyObject,
}

impl PythonModel {
    fn new() -&gt; PyResult&lt;Self&gt; {
        Python::with_gil(|py| {
            // Import the Python model
            let pickle = py.import("pickle")?;
            let io = py.import("io")?;
            let torch = py.import("torch")?;

            // Load the model from disk
            let model_file = std::fs::read("model.pkl")?;
            let bytes_io = io.call_method1("BytesIO", (model_file,))?;
            let model = pickle.call_method1("load", (bytes_io,))?;

            // Set model to evaluation mode
            model.call_method0("eval")?;

            Ok(Self { model })
        })
    }

    fn predict(&amp;self, features: Vec&lt;f32&gt;) -&gt; PyResult&lt;Vec&lt;f32&gt;&gt; {
        Python::with_gil(|py| {
            // Convert input to PyTorch tensor
            let torch = py.import("torch")?;
            let input = torch.call_method1(
                "tensor",
                (features,),
                Some([("dtype", torch.getattr("float32")?)]
                    .into_py_dict(py)),
            )?;

            // Add batch dimension
            let input = input.call_method0("unsqueeze", (0,))?;

            // Run inference
            let locals = [("model", &amp;self.model), ("input", input)]
                .into_py_dict(py);

            let result = py.eval(
                "model(input).detach().numpy().tolist()[0]",
                None,
                Some(&amp;locals),
            )?;

            // Convert result back to Rust
            let output: Vec&lt;f32&gt; = result.extract()?;
            Ok(output)
        })
    }
}

// Request and response types
#[derive(Deserialize)]
struct PredictionRequest {
    features: Vec&lt;f32&gt;,
}

#[derive(Serialize)]
struct PredictionResponse {
    predictions: Vec&lt;f32&gt;,
}

// Web server with Python model
async fn predict(
    model: web::Data&lt;PythonModel&gt;,
    request: web::Json&lt;PredictionRequest&gt;,
) -&gt; impl Responder {
    match model.predict(request.features.clone()) {
        Ok(predictions) =&gt; HttpResponse::Ok().json(PredictionResponse { predictions }),
        Err(e) =&gt; {
            eprintln!("Prediction error: {}", e);
            HttpResponse::InternalServerError().finish()
        }
    }
}

#[actix_web::main]
async fn main() -&gt; std::io::Result&lt;()&gt; {
    // Initialize the Python model
    let model = match PythonModel::new() {
        Ok(model) =&gt; model,
        Err(e) =&gt; {
            eprintln!("Failed to initialize model: {}", e);
            return Ok(());
        }
    };

    let model_data = web::Data::new(model);

    // Start the web server
    HttpServer::new(move || {
        App::new()
            .app_data(model_data.clone())
            .route("/predict", web::post().to(predict))
    })
    .bind("127.0.0.1:8080")?
    .run()
    .await
}</code></pre></pre>
<h2 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h2>
<p>Rust’s ML ecosystem has grown considerably, offering new frameworks like Burn and Candle that provide high-performance alternatives for specific ML use cases. While Python remains the dominant language for ML and data science, Rust offers compelling advantages for performance-critical components and production deployments.</p>
<p>By combining Rust’s safety and performance with Python’s rich ecosystem, you can build ML systems that have the best of both worlds: the flexibility and ecosystem of Python for research and development, and the reliability and efficiency of Rust for production.</p>
<p>As the Rust ML ecosystem continues to mature, we can expect more powerful tools and frameworks to emerge, further strengthening Rust’s position in the ML and data science landscape.</p>
<p>🔨 <strong>Project: ML Prediction Service</strong></p>
<p>For this chapter’s project, we’ll build a complete ML prediction service that:</p>
<ol>
<li>Loads a model trained in Python</li>
<li>Provides a high-performance API for predictions</li>
<li>Handles data preprocessing and postprocessing</li>
<li>Includes monitoring and error handling</li>
</ol>
<p>This project will demonstrate how to combine Rust’s performance and safety with Python’s rich ML ecosystem to create a production-ready ML service.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../chapters/41-distributed-systems.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../chapters/43-embedded.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../chapters/41-distributed-systems.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../chapters/43-embedded.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
